
replicaCount: 1

# Resource allocation per pod
resources:
  requests:
    cpu: 1
    memory: 4Gi
  limits:
    cpu: 2
    memory: 8Gi

# # Persistent volume for storing models
persistence:
  enabled: false

# Service configuration
service:
  type: NodePort

# Optional: limit logging or customize other features
logging:
  level: info

ollama:
  # -- Automatically install Ollama Helm chart from https://otwld.github.io/ollama-helm/. Use [Helm Values](https://github.com/otwld/ollama-helm/#helm-values) to configure
  enabled: true
  # -- If enabling embedded Ollama, update fullnameOverride to your desired Ollama name value, or else it will use the default ollama.name value from the Ollama chart
  fullnameOverride: "open-webui-ollama"
  # -- Example Ollama configuration with nvidia GPU enabled, automatically downloading a model, and deploying a PVC for model persistence
  ollama:
    # gpu:
    #   enabled: true
    #   type: 'nvidia'
    #   number: 1
    models:
      pull:
        - llama2
      run:
        - llama2


pipelines:
  # -- Automatically install Pipelines chart to extend Open WebUI functionality using Pipelines: https://github.com/open-webui/pipelines
  enabled: false

ingress:
  enabled: true
  class: alb
  annotations: 
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/group.name: webui
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}, {"HTTPS":443}]'  
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:165672952247:certificate/20668381-5373-415e-b8c5-2f589f909556
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/success-codes: 200,404
  host: 'open-webui.sanketlatkar.cloud'
